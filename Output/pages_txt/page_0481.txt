The cognitive processes we bring to moral reasoning aren’t perfect,
in that there are fault lines of vulnerability, imbalances, and
asymmetries.3 For example, doing harm is worse than allowing it—for
equivalent outcomes we typically judge commission more harshly than
omission and must activate the dlPFC more to judge them as equal. This
makes sense—when we do one thing, there are innumerable other things
we didn’t do; no wonder the former is psychologically weightier. As
another cognitive skew, as discussed in chapter 10, we’re better at
detecting violations of social contracts that have malevolent rather than
benevolent consequences (e.g., giving less versus more than promised).
We also search harder for causality (and come up with more false
attributions) for malevolent than for benevolent events.
This was shown in one study. First scenario: A worker proposes a
plan to the boss, saying, “If we do this, there’ll be big profits, and we’ll
harm the environment in the process.” The boss answers: “I don’t care
about the environment. Just do it.” Second scenario: Same setup, but this
time there’ll be big profits and benefits to the environment. Boss: “I
don’t care about the environment. Just do it.” In the first scenario 85
percent of subjects stated that the boss harmed the environment in order
to increase profits; however, in the second scenario only 23 percent said
that the boss helped the environment in order to increase profits.4
—
Okay, we’re not perfect reasoning machines. But that’s our goal, and
numerous moral philosophers emphasize the preeminence of reasoning,
where emotion and intuition, if they happen to show up, just soil the
carpet. Such philosophers range from Kant, with his search for a
mathematics of morality, to Princeton philosopher Peter Singer, who
kvetches that if things like sex and bodily functions are pertinent to
philosophizing, time to hang up his spurs: “It would be best to forget all
about our particular moral judgments.” Morality is anchored in reason.5
480