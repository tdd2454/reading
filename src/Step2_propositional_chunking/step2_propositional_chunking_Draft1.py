import json
import time
from tqdm import tqdm
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- C·∫§U H√åNH ---
INPUT_FILE = "behave_full_content.json"
OUTPUT_FILE = "behave_propositions.json"
MODEL_NAME = "qwen2.5:3b"  # Ho·∫∑c "qwen2.5:7b"
TEST_MODE = True  # True: Ch·ªâ ch·∫°y th·ª≠ 3 chunk ƒë·∫ßu ti√™n ƒë·ªÉ test. False: Ch·∫°y c·∫£ s√°ch.

def setup_llm_chain():
    """
    Thi·∫øt l·∫≠p LangChain v·ªõi Qwen model v√† Prompt chuy√™n d·ª•ng.
    """
    llm = ChatOllama(model=MODEL_NAME, temperature=0, format="json")
    
    # Prompt k·ªπ thu·∫≠t ƒë·ªÉ t√°ch √Ω v√† x·ª≠ l√Ω ƒë·∫°i t·ª´
    system_prompt = """
    You are an expert Knowledge Graph data pre-processor.
    Your task is to decompose the given text into "Atomic Facts" (short, standalone sentences).

    STRICT RULES:
    1. Split compound sentences into simple sentences.
    2. RESOLVE COREFERENCES: Replace pronouns (it, he, she, they, this, that) with the specific entities they refer to.
    3. Maintain original scientific terminology.
    4. Output MUST be a valid JSON list of strings.
    5. Do not add any explanation.

    Example Input: "The amygdala receives input from the cortex, but it relies on the PFC for regulation."
    Example Output: ["The amygdala receives input from the cortex.", "The amygdala relies on the PFC for regulation."]
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{text}")
    ])
    
    # Chain: Prompt -> LLM -> JSON Parser
    return prompt | llm | JsonOutputParser()

def split_text_into_windows(text):
    """
    C·∫Øt text th√†nh c√°c ƒëo·∫°n nh·ªè v·ª´a ph·∫£i ƒë·ªÉ LLM x·ª≠ l√Ω (kho·∫£ng 3-5 c√¢u).
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,      # ƒê·ªß l·ªõn ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh 1 ƒëo·∫°n vƒÉn
        chunk_overlap=150,    # Overlap ƒë·ªÉ kh√¥ng b·ªã ƒë·ª©t m·∫°ch √Ω ·ªü bi√™n
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    return splitter.split_text(text)

def process_chapter(chapter_data, chain):
    """
    X·ª≠ l√Ω m·ªôt ch∆∞∆°ng: C·∫Øt nh·ªè -> G·ª≠i LLM -> Gom l·∫°i.
    """
    title = chapter_data['title']
    raw_content = chapter_data['content']
    
    # 1. C·∫Øt nh·ªè content th√†nh c√°c windows
    windows = split_text_into_windows(raw_content)
    
    # if TEST_MODE:
    #     print(f"   [TEST MODE] Ch·ªâ x·ª≠ l√Ω 3 ƒëo·∫°n ƒë·∫ßu c·ªßa ch∆∞∆°ng '{title}'...")
    #     windows = windows[:3]

    chapter_propositions = []
    
    # 2. Loop qua t·ª´ng window ƒë·ªÉ x·ª≠ l√Ω
    for i, window in enumerate(tqdm(windows, desc=f"   Processing {title}", leave=False)):
        try:
            # G·ªçi LLM
            result = chain.invoke({"text": window})
            
            # Result k·ª≥ v·ªçng l√† list strings: ["fact 1", "fact 2"]
            if isinstance(result, list):
                # L∆∞u th√™m metadata ngu·ªìn g·ªëc
                for prop in result:
                    chapter_propositions.append({
                        "text": prop,
                        "source_chapter": title,
                        "original_window_index": i
                    })
            else:
                # Fallback n·∫øu LLM tr·∫£ v·ªÅ format l·∫° (hi·∫øm g·∫∑p v·ªõi Qwen)
                chapter_propositions.append({"text": window, "source_chapter": title, "note": "raw_fallback"})
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói chunk {i}: {e}")
            continue
            
    return chapter_propositions

def main():
    print(f"--- B·∫ÆT ƒê·∫¶U PROPOSITIONAL CHUNKING (Model: {MODEL_NAME}) ---")
    
    # 1. Setup
    try:
        chain = setup_llm_chain()
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi Ollama: {e}")
        print("üëâ H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ ch·∫°y 'ollama serve' v√† 'ollama pull qwen2.5:3b'")
        return

    # 2. Load d·ªØ li·ªáu
    try:
        with open(INPUT_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file input '{INPUT_FILE}'.")
        return

    all_propositions = []
    
    # 3. Process t·ª´ng ch∆∞∆°ng
    # N·∫øu TEST_MODE = True, ch·ªâ ch·∫°y ch∆∞∆°ng ƒë·∫ßu ti√™n
    chapters_to_process = data[6:8] if TEST_MODE else data
    
    for chapter in tqdm(chapters_to_process, desc="Total Progress"):
        props = process_chapter(chapter, chain)
        all_propositions.extend(props)

    # 4. Save Output
    final_output_file = OUTPUT_FILE if not TEST_MODE else "test_behave_propositions.json"
    
    with open(final_output_file, "w", encoding="utf-8") as f:
        json.dump(all_propositions, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ HO√ÄN T·∫§T! T·ªïng c·ªông {len(all_propositions)} m·ªánh ƒë·ªÅ.")
    print(f"üëâ K·∫øt qu·∫£ l∆∞u t·∫°i: {final_output_file}")
    
    if TEST_MODE:
        print("\n--- PREVIEW K·∫æT QU·∫¢ (5 d√≤ng ƒë·∫ßu) ---")
        print(json.dumps(all_propositions[:5], indent=2, ensure_ascii=False))
        print("\nüí° N·∫øu k·∫øt qu·∫£ t·ªët, h√£y ch·ªânh TEST_MODE = False ƒë·ªÉ ch·∫°y full s√°ch (s·∫Ω t·ªën v√†i gi·ªù).")

if __name__ == "__main__":
    main()